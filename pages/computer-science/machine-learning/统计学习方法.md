- 绪
	- 由于时间原因（只预留两周复习机器学习），所以暂时复习统计学习方法的第二章到第十章
	- CS229未能看完T^T，半途而废，前功尽弃。。。。
	- 西瓜书也搁置T^T，几十大洋，白白浪费
	- 此文大量参考：<a  href="https://github.com/SmirkCao/Lihang">https://github.com/SmirkCao/Lihang</a> ；感谢大佬做的贡献
- Ch2 感知机
	- 模型三要素
		- 模型 ![image.jpg](../assets/58b7bd24-e148-4ada-b5f8-359cd6a6bf66-1115003.jpg)
		- 策略
			- 确定学习策略就是定义**(经验)**损失函数并将损失函数最小化。
			- 注意：这里提到了经验，所以学习是base在训练数据集上的操作 **//这里有点不懂**
		- 损失函数，其中M代表误分类的点 ![image.jpg](../assets/0fe1d2c7-9657-4edf-bf21-778dbc1bb5a5-1115003.jpg)
			- 其中L是w和b的可导函数
	- 学习算法
		- 原始形式
			- 输入 ![image.jpg](../assets/e7707fae-d2ce-421f-bd4f-f129cee27837-1115003.jpg)
			- 输出 ![image.jpg](../assets/2a570606-fe7d-43e2-83bf-89786c0e4bfa-1115003.jpg)
				- 迭代公式3，实际上便是梯度下降
				- 可以对x向量补额外1，将w和b的更新合并在一起，合在一起的这个叫做扩充权重向量。
		- 对偶形式：将**w**和**b**表示为**xi**和**yi**的线性组合形式
			- 由原始形式，可得w（假设w初值为0，αi是指线性感知机在第i个点中，分类错误的次数*μ） ![image.jpg](../assets/e5d111b1-fcdb-40b2-9cf4-41f7282a1662-1115003.jpg)
			- 故可得对偶版本 ![image.jpg](../assets/c51575b5-b6d3-4de9-87b8-76a8dded150d-1115003.jpg)
			- 在对偶版本中，训练实例仅以内积的形式出现。为了方便可以先将内积计算出来，并以矩阵形式存储 ![image.jpg](../assets/7f4b5c92-f873-44ce-9693-17e57a70c311-1115003.jpg)
- Ch3  KNN(关键构造KDT tree来搜索最近邻)
	- KNN算法
		- 输入 ![image.jpg](../assets/0a571ab4-06bc-442a-88ac-7f4b2a7b1709-1115003.jpg)
		- 输出：实例所属于的类别y
		- 步骤：找到k个最近的点，并根据这k个点投票决定类别 ![image.jpg](../assets/f88970d1-4441-4557-b317-c31c161eed4d-1115003.jpg)
	- 距离度量
		- 特征空间中的两个实例点的距离是两个实例点相似程度的反映。
		- 公式（p = 2就是欧式距离，p = 1就是曼哈顿距离） ![image.jpg](../assets/1784f7e2-b0aa-469b-80ac-e60914ad41e8-1115003.jpg)
	- KDTree（使得搜索的效率更高）（这里的K指的是对k维空间的划分）
		- 构造KD tree（k dimension tree）：本质上是对k维空间的一个划分（类似于二叉搜索树对1维空间的划分），详见 <a  href="https://blog.csdn.net/guoziqing506/article/details/54692392">https://blog.csdn.net/guoziqing506/article/details/54692392</a>
			- 这里划分的方法是单独的每一个维度进行划分：具体方法如下
				- 假设向量X = [x1,x2,...,xk]有k个维度，有n个X向量构成集合U，我们要对U进行划分
					- 选取一个维度（方差最大的，也可以按照1、2、3...k的顺序，）的中位数，按照这个维度将集合切分成两个子集和（即在这个特定维度的二分）
					- 选取下一个维度（方差最大的，或者按顺序）继续划分
					- 直到所有的集合都只剩一个元素，不能继续划分
		- 搜索KD tree：有节点S，找出KD tree中与S节点最近的点
			- 找到叶子节点
				- util T 是叶子节点
					- 假设T的划分维度是k：if S[k] > T[k]: T = T.right
					- else : T = T.left
			- 将**叶子节点T**作为S的**距离最近点**
			- 递归回退操作：
				- 计算T的S的最近距离**mindis**
				- 查看以**S 为原点，** mindis为半径的圆是否包括另一子树：即该父节点另一子树是否包含更近的点
				- 包括的话就搜索另一个子树，找到另一个最近点
				- 否则继续向上回溯，直到根节点（因为父节点的父节点的另一子树可能存在更近的点）
			- 例子： ![image.jpg](../assets/cce1e95b-ea71-4c0b-8a66-8640ae65aca6-1115003.jpg)
				- 上图是一个2维的KD 树 ![image.jpg](../assets/1132ce20-697d-467d-a475-758f651076df-1115003.jpg)
				- 找到叶子节点D，然后dis(S,D)作为最小距离
				- 回溯至父节点B发现，B的另一子树F不和，**S-D圆相交**
				- 继续向上回溯节点A，发现E和**S-D圆相交**，将E做为最新的最近节点
				- 根节点结束
- Ch4  朴素贝叶斯（生成模型）
	- 算法
		- 输入空间 **X**∈R^n：**X**是n为向量,**X** = {x1,x2...xn}，其中训练集U = {**X1...Xn**}
		- 输出空间Y = {c1，c2...，cn}
			- 先验概率P(Y)
		- 求得事件为X条件概率P(X|Y) = P(x1|Y)P(x2|Y)...P(xn|y)
			- 条件独立假设
				- 对于给定Y而言，对于X中的n个事件，它们是独立的 ![image.jpg](../assets/d812b14d-93c2-46fc-9784-a5b7c9b248b0-1115003.jpg)
					- 推导（假设X有X1 X2） ![image.jpg](../assets/06d81e6d-cb63-481c-af39-8c49acf4a0fc-1115003.jpg)
		- 通过参数估计极大似然法（样本为训练集），估计总体的P(Y),P(X|Y)
			- 总体P(Y)估计 ![image.jpg](../assets/19037e0f-dedd-494e-b255-0e4080629405-1115003.jpg)
			- 总体P(X|Y估计) ![image.jpg](../assets/bdc8ec8f-ed53-408b-8ad0-6fb05872e888-1115003.jpg)
				- 拉普拉斯平滑：即当x不存在的时候，不一定代表概率为0，也许只是概率狠下
		- 对给定的未知样本X，通过贝叶斯求得后验概率P(Y|X)，则最大P(Y = cj|X)即为预测的X的类别 ![image.jpg](../assets/cd07afa9-d5f9-494b-a2d1-2c109144fcfa-1115003.jpg)
			- 贝叶斯 ![image.jpg](../assets/73e8cfc6-bd39-4d54-a4c0-8426d5f1293a-1115003.jpg)
			- 条件独立假设 ![image.jpg](../assets/f226998b-4455-4136-9c4b-cf05aee9b4f1-1115003.jpg)
			- 取最大的作为类别 ![image.jpg](../assets/ae0a071a-58d6-4285-8c66-5cf0826ff6c4-1115003.jpg)
			- 由于分母相同 ![image.jpg](../assets/bfb1035d-12c2-4880-9d47-54756d7886dd-1115003.jpg)
		- 后验概率最大的意义
			- 假设决策函数是f(X),则损失函数L为 ![image.jpg](../assets/1a053dad-e301-4114-a31a-be5b3c590e25-1115003.jpg)
			- 推得期望为，**期望是对联合分布取的？？？** ![image.jpg](../assets/44f863ca-17d7-470a-936d-1c8baef14f27-1115003.jpg)
	- 书中的贝叶斯流程 ![image.jpg](../assets/08b1084c-e188-4770-8d97-d65713813bbe-1115003.jpg)
- Ch5 决策树
	- 基本概念
		- 熵：**熵只与的分布有关，与X取值无关**（即意味着信息熵本质是由分布/概率 决定的）<img src="https://api2.mubu.com/v3/document_image/583dcfc7-1c9a-4619-9038-1bc8edcef97f-1115003.jpg" /> ![image.jpg](../assets/9faabc33-a862-453b-86b0-272c28131a3e-1115003.jpg)
		- 条件熵：条件熵H(Y|X)表示在**已知随机变量X的条件**下随机变量Y的**不确定性** ![image.jpg](../assets/924e361d-d1bb-4b98-b7fb-b03ea7530a6e-1115003.jpg)
		- 经验熵
			- 经验熵：熵的概率是由极大似然估计得到
			- 经验条件熵：条件熵中的概率由数据估计(特别是极大似然估计)得到
		- **信息增益：特征引入A带来信息熵的减少（知道了一个新的信息，不确定减少，信息熵肯定减少）** ![image.jpg](../assets/22cb624f-18bf-4cc6-938c-6eee21b744ba-1115003.jpg)
			- 算法 ![image.jpg](../assets/a35863c5-325d-4751-8dd0-0c1ee27af55f-1115003.jpg)
				- 训练集D
				- Ck，表示X=k（随机变量为k）的集合
				- Di，表示满足特征A（附i表示第i个子条件）的集合，n表示A包含n个条件，集有n个取值
		- **信息增益比：**特征引入带来了信息增益与**原特征熵**的比（HA(D)是D集合中A的信息增益） ![image.jpg](../assets/7de89723-2f36-4800-ba4f-0f24b57601ea-1115003.jpg)
			- HA(D)计算方法 ![image.jpg](../assets/146ae9b1-ae64-447e-b9cd-e75dcaaa9c90-1115003.jpg)
				- 表示满足特征A（附i表示第i个子条件）的集合
	- 决策树算法：决策树每一个节点的子树表示**按照某个特征**对 **特征空间的划分**
		- 过程：递归了对特征空间的划分
			- 构建根节点，将所有数据放在根节点
			- 选择一个**最优特征，**按照这一特征将数据**分割**成子集**，**使得子集在当前条件下有**最好分类**
			- 构建叶子节点，并将这些子集分配到对应叶子节点
			- 如果有子集**不能被正确分类**，递归的对这些子集选最优特征，继续分割
			- 直到所有子集被基本正确的分类，或者没有合适的特征
		- 算法
			- 算法5.2 ID3算法
				- 输入：训练数据集和D，特征集A，阈值ε
				- 输出：决策树T
				- 流程
					- if： D 属于同一类Ck，
						- T是单节点树，类Ck作为该节点的标记
						- return T
					- if：A是空集
						- T是单节点树，实例树最多的类作为该节点的标记
						- return T
					- else：选择特征
						- 计算每个A中各个特征对应的信息增益G(Ai,D),选择学校增益最大的Ag（是选择一个含有信息最多的特征）
						- if G(Ag,D) < ε
							- T为单节点树，D中实例数最多的类Ck作为标记，返回T
						- else
							- 按照特征Ag，将D划分成若干子集Di
							- 选取Di中实例数最大的类作为标记，构建子节点
					- 以Di为训练集，A = A-Ag(去掉特征Ag)，递归的调用上述流程
			- 算法 C4.5生成（附C4.5和ID3的区别：ID3使用信息增益判断，C4.5使用信息增益比判断）
				- 输入：训练数据集和D，特征集A，阈值ε
				- 输出：决策树T
				- 流程
					- if： D 属于同一类Ck，
						- T是单节点树，类Ck作为该节点的标记
						- return T
					- if：A是空集
						- T是单节点树，实例树最多的类作为该节点的标记
						- return T
					- else：选择特征
						- 计算每个A中各个特征对应的信息增益比gR(Ai,D),选择信息增益比最大的Ag（是选择一个信息增益比的特征）
						- if gR(Ag,D) < ε
							- T为单节点树，D中实例数最多的类Ck作为标记，返回T
						- else
							- 按照特征Ag，将D划分成若干子集Di
							- 选取Di中实例数最大的类作为标记，构建子节点
					- 以Di为训练集，A = A-Ag(去掉特征Ag)，递归的调用上述流程
			- 树的剪枝
				- 剪枝是通过极小化决策树整体的损失函数来实现
					- 损失函数：
						- 树T的叶子节点的个数|T|（叶子节点即最后分类的类）
						- 对于叶子节点t，该节点有Nt个样本，其中k类的样本个数Ntk
						- Ht|T|为叶子节点t上的经验熵（是根据这个样本来求概率的（总体中类k的概率不一定是这样），所以叫做经验熵）； ![image.jpg](../assets/f1f252f5-04da-4c5d-8b23-251a885096f7-1115003.jpg)
							- 由于决策树可能再特征A用完的时候结束剪枝，所以这里熵不一定为0，剪枝之后的肯定不为0
						- 则针对所有叶子节点 ![image.jpg](../assets/78220d0c-06f4-4f2f-938c-b416ae5b00e3-1115003.jpg)
							- 乘以Nt
								- 1，交叉熵只考虑了分布（结果在0~1之间），没有考虑叶子节点样本的数目（叶子节点样本数目越多，对应交叉熵在损失函数中的权重越大）
								- 2，参考 [<a  href="https://mrtriste.github.io/2017/08/07/%E5%86%B3%E7%AD%96%E6%A0%91%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0Nt%E7%9A%84%E7%90%86%E8%A7%A3/">https://mrtriste.github.io/2017/08/07/%E5%86%B3%E7%AD%96%E6%A0%91%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0Nt%E7%9A%84%E7%90%86%E8%A7%A3/</a>]，感觉有点扯
									- 就是熵代表样本中每个节点不确定性的期望，那么对于叶子节点的样本整体，熵的总和是Nt*Ht，对应整棵树|T|，即所有样本的熵的总和的和
						- 再加上一个对叶子节点数的修正（正则化），叶子节点越大，惩罚越大（α非负） ![image.jpg](../assets/567ac2c7-250a-4d1a-a312-3d033700c10c-1115003.jpg)
						- 附：
							- C(T)表示的是叶子节点熵，越小说明预测的越准确
							- |T|越小表示树越简洁
							- 其中α表示对树复杂性惩罚的力度
					- 算法流程
						- 输入：生成的树T，参数α
						- 输出：修剪后的树Tα
							- 递归的剪枝
								- 计算每个节点的经验熵
								- 计算损失函数Cα(TB)
								- 计算假设减去某个叶子节点的损失函数Cα(TA)
								- if Cα(TA) <= Cα(TB)，剪枝
								- 递归，直到不能剪枝
		- CART：classification and regression tree
			- CART假设决策树是二叉树。内部节点的取值为：是 or 否
			- 回归树的生成
				- 对于回归模型
					- 训练集合 {(x1,y1),(x2,y2),(x3,y3)}
				- 首先在训练集中找到一个变量xj，对于它的取值s 满足如下等式（即使得按照xj = s划分的两个子集合的方差最小） ![image.jpg](../assets/ff7bd581-5c61-4e61-872d-a7d6acef14fe-1115003.jpg)
					- c 为 该子集合 标签 yi 的平均值，Rm即为按照(xj,s)划分的子集和,对上式求导可得。 ![image.jpg](../assets/369d3f3e-f38d-4519-ad81-753c88fa9217-1115003.jpg)
				- 递归划分，最后叶子节点取值即为为使得平方误差最小的值（即该叶子节点样本的均值）
			- 算法5.5 最小二乘回归树生成
				- 输入：训练数据集D
				- 输出：回归树f(x)
				- 步骤：
					- 遍历变量j，对固定的切分变量j扫描切分点s（xj = s），得到以下关系 ![image.jpg](../assets/68f602f3-5864-4624-b4ae-4f32186cf5e6-1115003.jpg)
					- 用选定的**（j，s），**两个字集合并决定其输出值（N是Rm集合的yi的数量） ![image.jpg](../assets/d72ad3b5-44d5-4ed3-b1af-296f9d4a63d8-1115003.jpg)
					- 递归调用 1、2 步骤，直到满足停止条件（样本内的x无法划分）（在分类树中有特征用完停止划分，然而在回归树中x就是特征，属于不可能划分完，除非只有一个变量）
			- 算法5.6 CART分类树的生成
				- 基尼指数
					- 基尼指数的公式定义，（pk = 0.5最大，和熵一样，都是用来衡量集合D的不确定性） ![image.jpg](../assets/5773b1d5-fb77-4526-aa00-96e55480ada2-1115003.jpg)
						- 基尼指数Gini(D,A) 和 熵的一般H(p)/2 和分类误差类别的关系 （横轴表示p，纵轴表示损失？？） ![image.jpg](../assets/2a3c54ae-5e64-42f6-b6d0-c851252b7c0e-1115003.jpg)
					- 二分类的基尼指数 ![image.jpg](../assets/e9e475b0-0cdb-45ba-8e4c-1c75e8ed43af-1115003.jpg)
					- 对于集合D，其基尼系数，其中K表示**K个类别** ![image.jpg](../assets/97c1de40-9435-4b85-aae4-10b14d9a22c4-1115003.jpg)
					- 在特征A的条件下，集合D的基尼系数为： ![image.jpg](../assets/53925f54-7731-44ed-b7f4-453bbc29b1e3-1115003.jpg)
				- 则根据基尼系数的决策树生成算法
					- 对每一个特征A可能的取值a，将是否A=a作为条件将D划分为D1 D2，计算该特征的基尼系数 ![image.jpg](../assets/0513da96-8e71-429d-a59d-a9aefd5ddb1b-1115003.jpg)
					- 选取基尼系数最小的特征a作为切分点（本质上还是和上述一样，选择该特征，使得两个子集合的混乱程度最低）
					- 递归1 2，直至满足条件
						- 特征用完
						- 分无可分
				- 剪枝
					- 剪枝的核心1：最小化损失函数（即每一个节点的     熵*节点个数   +  常数） ![image.jpg](../assets/5d824e57-8bb3-49cf-ab9a-7007224756c0-1115003.jpg)
					- 其中关键是如何选取α，α可以取无穷个数
						- **key point1**:树的T的子树是有限的 {T0，T1，T2，T3...Tn}，可以通过交叉验证取得最优的子树
						- **key point2：**其中每一个子树Ti，肯定对应一个αi取值（**即对于一个α取值**（取值范围？），**Ti是最优的**（α = 0.完全子树最优，α = 无穷，根节点最优））
							- 求解过程
								- 假设有一个节点t，其子树为Tt
									- 减去该子树后的损失函数：（**只包含 **t 节点 不确定性 加上  常数α） ![image.jpg](../assets/853afdc9-b8f3-42fb-95c3-b42d3fc50db2-1115003.jpg)
									- 减去该子树之前的损失函数：（即整个子树的损失，针对子树的每个叶子节点） ![image.jpg](../assets/2e64cc4c-c332-4779-9fa7-9fff278976bf-1115003.jpg)
									- 其中α对剪枝的影响：（即代表当x = 特定值，可以使得剪枝成功）
										- 当α=0或足够小，减去一个子树，不确定性增加（即决策树本身就是一个减少原数据集不确定性的操作）（不需要剪枝） ![image.jpg](../assets/0cef7a72-9ade-456e-a48a-bf9f59aaffb8-1115003.jpg)
										- 当α增大，增大的不确定性 和 减小的常数误差项**抵消**，如此使得：此时得到一个剪枝的临界值（即在这种情况，是剪枝的临界点（虽然损失函数没有减少，但是模型更简单了？减少过拟合，所以剪枝）） ![image.jpg](../assets/177035f0-9fc3-4306-98e6-9e73983d5d5b-1115003.jpg)
										- α足够大（无穷大），增大的不确定性 小于 减小的常数误差项（一定要剪枝） ![image.jpg](../assets/ffc5e3f9-90e8-4934-a36a-10e0e6c3fa65-1115003.jpg)
										- **结论：当α变大的时候，需要剪枝的节点越来越多，在  [αi-1，αi)  的范围中，Ti是最优的** ![image.jpg](../assets/6bc07caa-1793-429c-aa16-dd7a90a02b6b-1115003.jpg)
											- **当大于等于αi时，需要减去ti+1，子树变成了Ti+1**
											- **当小于αi-1时，不需要减去ti，子树变成了Ti-1**
						- **key point3：**现在直到了需要生成子树序列，需要求出对应的α，那么该按照什么顺序减去子树，得到子树序列{T0，T1，T2，T3...Tn}
							- 由key point2得到，α 是关于t的一个函数（即针对每一个子树的根节点t，都有一个α，该α是保证减去该子树的最小值），按照之前我们将α从小到大遍历所有子树序列（子树序列是相互包含，即 αt   对应的子树，减去了所有小于t的子树）<img src="https://api2.mubu.com/v3/document_image/a69d2204-41d2-42ab-9b80-8dc56b7a89c3-1115003.jpg" /> ![image.jpg](../assets/17c03051-0f7e-4da6-8b34-94aed423633c-1115003.jpg)
				- 其具体算法描述为 ![image.jpg](../assets/ee20320e-e78d-467f-bc9e-b4fccea8f083-1115003.jpg)
- Ch6 逻辑回归（判别模型）？？？？
	- 对数几率回归(逻辑斯谛回归)：这里从概率的角度理解（网上有从函数拟合角度）
		- logistic distribution：
			- 概率函数，这就是我们常见的sigmoid函数（将x映射到0~1之间），其中μ是位置参数（1/2位置），γ是形状参数，决定曲线的陡峭程度 ![image.jpg](../assets/37f3fad9-4b48-45dc-9802-2c5ce5152cbe-1115003.jpg)
		- 二项 logistic distribution：
			- 由于x满足logistic distribution，所以针对Y = 1的条件下，定义 x 的分布为一种logistic 分布 ![image.jpg](../assets/6b5d141c-af67-482d-ad0a-1ec128c204a4-1115003.jpg)
			- Y = 0的情况下，即1-P(Y=1|x)得到下式 ![image.jpg](../assets/085bed91-1536-4ef1-b6ca-8ad34cdbacba-1115003.jpg)
			- 比较P（Y=1|x)）和 P（Y=0|x）的大小即可以得到x对应的Y（有点类似贝叶斯，区别是这里模型参数θ是确定的，通过极大似然估计，但贝叶斯的思想是θ是参数，具体详见CS229）
				- 附：贝叶斯思想：<a  href="https://www.zhihu.com/question/21134457">https://www.zhihu.com/question/21134457</a> ![image.jpg](../assets/a507e8f7-86ff-458c-b72d-1a64c3221922-1115003.jpg)
			- 通过极大似然函数估计θ
				- 极大似然函数 ![image.jpg](../assets/30570a39-e9f3-445c-8657-a9faaa7bc0f5-1115003.jpg)
				- 对数化 ![image.jpg](../assets/f2bee565-12fb-4a2a-a674-9be1bf6efbb4-1115003.jpg)
				- 对对数似然函数概率最大化（对数似然函数即为损失）
		- 附：多项便成了softmax
	- 最大熵模型：在已知条件的约束下，任意未知条件的（发生的可能性，权重，几率？）是等可能的，即熵最大
		- 定义约束条件
			- 特征函数：描述分布的特征，在这里表示的是（x，y）的某个特征，（比如（太阳、晴天）= 1,（太阳，阴天）= 0，即表示有这一个约束） ![image.jpg](../assets/2b3498ec-4024-4081-85de-114d28215da8-1115003.jpg)
			- 定义特征函数的期望： ![image.jpg](../assets/25a14939-007f-4835-9643-9400cbf2fbb0-1115003.jpg)
			- 则我们定义的约束是：样本特征函数期望(~p) 和 总体特征函数期望相同 ![image.jpg](../assets/9366ff4c-4a88-48e7-8df7-e3f237f0efaa-1115003.jpg)
		- 优化函数
			- 条件熵
				- 在已知x的条件下，y的概率为P(y|x)，则其条件熵为： ![image.jpg](../assets/61f0fd09-7616-4a4d-bbba-e14eb7102dd2-1115003.jpg)
			- 最大熵的优化函数就是使得总体空间的条件熵最大（附即负数的最下），（这里外乘了一个~p(x),概率越高权重越大？）（p(x|y)应该是有一个先验函数的） ![image.jpg](../assets/4f18cb72-885a-4a90-9837-98a48c05f1bd-1115003.jpg)
		- 优化方法：拉格朗日乘子法（乘子：w0...wn）
			- 拉格朗日函数：L(P,w) ![image.jpg](../assets/0af450ec-0d38-4a0d-8a4a-7770c1397a13-1115003.jpg)
			- L(P,w)是凸函数
- SVM
	- 线性可分支持向量机和
- 提升方法
- EM